{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "def getData():\n",
    "    data = load_svmlight_file(\"C:/Users/YOUNA/Desktop/a9a.txt\")\n",
    "    return data[0], data[1]\n",
    "def model(weigth,b,x):\n",
    "    return x.dot(weight)+b;\n",
    "\n",
    "def loss(weight,b,x,y,C):  \n",
    "    weightLoss = (weight**2).sum()/2;\n",
    "    pointLoss = 0;\n",
    "    n = x.shape[0]\n",
    "    for i in range(n):\n",
    "        value = y[i]*(model(weight,b,x[i]))\n",
    "        if (1-value )>=0:\n",
    "            pointLoss=pointLoss+C*(1-value)/n      \n",
    "    return weightLoss + pointLoss\n",
    " \n",
    "def gradiendofW(weight,b,x,y,C): \n",
    "    WG=weight\n",
    "    n = x.shape[0]\n",
    "    for i in range(n):\n",
    "        value = y[i]*(model(weight,b,x[i]))\n",
    "        if (1-value )>=0:\n",
    "            WG=WG-y[i]*x[i]*C/n; \n",
    "    return WG\n",
    "\n",
    "def gradiendofB(weight,b,x,y,C): \n",
    "    WB=0\n",
    "    n = x.shape[0]\n",
    "    for i in range(n):\n",
    "        value = y[i]*(model(weight,b,x[i]))\n",
    "        if (1-value )>=0:\n",
    "            WB=WB-y[i]*C/n;    \n",
    "    return WB\n",
    " \n",
    "\n",
    "def RMSprop(grad,learningRate,dampingRate,stabilization):\n",
    "    r=(1-dampingRate)*(grad**2)\n",
    "    delta=grad\n",
    "    for i in range(grad.shape[0]):\n",
    "        delta[i]=-1*learningRate*grad[i]/(stabilization+math.sqrt(r[i]*1.0))                                   \n",
    "    return delta\n",
    "\n",
    "def Adam(weight,step,stabilization,stampingRate1,stampingRate2):\n",
    "    s=numpy.zeros((1,weight.shape[0]))\n",
    "    r=numpy.zeros((1,weight.shape[0]))\n",
    "    s=stampingRate1*s+(1-stampingRate1)*weight\n",
    "    r=stampingRate2*r+(1-stampingRate2)*weight**2\n",
    "    vs=s/(1-stampingRate1)\n",
    "    vr=r/(1-stampingRate2)\n",
    "    deltaofW=weight\n",
    "    for i in range(weight.shape[0]):\n",
    "        deltaofW[i]=-step*vs[0][i]/(math.sqrt(vr[0][i])+stabilization)\n",
    "    return deltaofW\n",
    "\n",
    "def NAG(weight,preUpdate,miu,alpha,x,y,C,b):\n",
    "    newWeight=weight+miu*preUpdate\n",
    "    WG=newWeight\n",
    "    n = x.shape[0]\n",
    "    for i in range(n):\n",
    "        value = y[i]*(model(newWeight,b,x[i]))\n",
    "        if (1-value )>=0:\n",
    "            WG=WG-y[i]*x[i]*C/n;\n",
    "    update = miu*preUpdate-alpha*WG\n",
    "    return update\n",
    "\n",
    "def AdaDelta(tparams,grads):\n",
    "    p=0.95;e=1e-6\n",
    "    delta_x2=[theano.shared(p.get_value() * floatX(0.)) for k, p in tparams.iteritems()]\n",
    "    g2 = [theano.shared(p.get_value() * floatX(0.)) for k, p in tparams.iteritems()]\n",
    "    update_g2=[(g2, p * g2 + (1-p) * (g ** 2)) for g2, g in zip(g2, grads)]\n",
    "    fn_update_1=theano.function(inputs=[],updates=update_g2)\n",
    "    delta_x=[-T.sqrt(delta_x2_last + e) / T.sqrt(g2_now + e) * g for g, delta_x2_last, g2_now in zip(grads,delta_x2,g2)]\n",
    "    update_delta_x2=[(delta_x2, p * delta_x2 + (1-p) * (delta_x ** 2)) for delta_x2, delta_x in zip(delta_x2, delta_x)]\n",
    "    update_param=[(param, param + delta) for param, delta in zip(tparams.values(), delta_x)]\n",
    "    fn_update_2=theano.function(inputs=[],updates=update_delta_x2+update_param)\n",
    "    return fn_update_2\n",
    "\n",
    "x, y = getData()\n",
    "x=x.toarray()\n",
    "for i in range(x.shape[0]):\n",
    "    if(y[i]==-1):\n",
    "         y[i]=0\n",
    "xTrain, xValidatuon, yTrain, yValiidation = train_test_split(x, y, test_size=0.5, random_state=42)\n",
    "weight = numpy.zeros(xTrain.shape[1])\n",
    "b=0;\n",
    "C=10;\n",
    "iteration = 100\n",
    "learningRate = 0.001\n",
    "trainLoss = []\n",
    "validateLoss = []\n",
    "preUpdate=numpy.zeros(xTrain.shape[1])\n",
    "preGrad=numpy.zeros(xTrain.shape[1])\n",
    "for i in range(iteration):\n",
    "    rand=[]\n",
    "    X=numpy.zeros((100,xTrain.shape[1]))\n",
    "    Y=numpy.zeros((100,))\n",
    "    for j in range(100):\n",
    "        rand.append(random.randint(0,xTrain.shape[0]-1))\n",
    "    for k in range(len(rand)):\n",
    "        X[k]=xTrain[rand[k]]\n",
    "        Y[k]=yTrain[rand[k]]\n",
    "        \n",
    "    gradofW = gradiendofW(weight,b,X,Y,C)  \n",
    "    gradofB = gradiendofB(weight,b,X,Y,C)\n",
    "   # weight = weight + RMSprop(gradofW,learningRate,0.1,0.0000001)\n",
    "   # weight = weight + Adam(gradofW,0.1,0.000000001,0.9,0.999)\n",
    "    weight = weight - gradofW*learningRate\n",
    "   # update=NAG(weight,preUpdate,0.01,0.9,X,Y,C,b)\n",
    "   # weight = weight+update\n",
    "    \n",
    "    b = b - gradofB*learningRate\n",
    "    trainLoss.append(loss(weight,b,xTrain,yTrain,C))\n",
    "    validateLoss.append(loss(weight,b,xValidatuon,yValiidation,C))\n",
    "    preUpdate=update\n",
    "    \n",
    "plt.plot(range(iteration),trainLoss, label='train loss')\n",
    "plt.plot(range(iteration),validateLoss, label='validtion loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
